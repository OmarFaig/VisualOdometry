\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{color}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{minted}
\usepackage{float}

\geometry{margin=2.5cm}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

\title{Monocular Visual SLAM System\\
\large Documentation}
\author{Visual SLAM Project Team}
\date{\today}

\begin{document}

\maketitle
\tableofcontents
\newpage

\section{Introduction}
This document provides comprehensive documentation for a Monocular Visual SLAM (Simultaneous Localization and Mapping) system. The system is designed to create both 2D and 3D maps of an environment using only a single camera.

\subsection{System Overview}
The system implements a feature-based visual SLAM approach that:
\begin{itemize}
    \item Tracks camera motion using visual features
    \item Reconstructs 3D points from 2D image correspondences
    \item Generates both 2D trajectory maps and 3D point clouds
    \item Supports both recorded datasets and live video input
\end{itemize}

\section{Installation}
\subsection{Dependencies}
The system requires the following dependencies:
\begin{lstlisting}[language=bash]
numpy>=1.19.4
opencv-python>=4.5.0
open3d>=0.13.0
matplotlib>=3.3.3
tqdm>=4.54.0
pathlib>=1.0.1
\end{lstlisting}

\subsection{Docker Deployment}
For deployment on Jetson Nano, a Docker container is provided. Build and run the container using:
\begin{lstlisting}[language=bash]
# Build the container
docker build -t visual_slam_jetson .

# Run the container
docker run --runtime nvidia \
           --network host \
           --device /dev/video0:/dev/video0 \
           -v /tmp/.X11-unix:/tmp/.X11-unix \
           -e DISPLAY=$DISPLAY \
           visual_slam_jetson
\end{lstlisting}

\section{System Architecture}
\subsection{Core Components}
The system consists of several key components:
\begin{itemize}
    \item Feature Detection and Matching
    \item Pose Estimation
    \item 3D Point Triangulation
    \item Scale Estimation
    \item Map Generation
\end{itemize}

\subsection{Class Structure}
The main \texttt{MonocularSlam} class contains the following key methods:
\begin{itemize}
    \item \texttt{process\_frame}: Main entry point for processing new frames
    \item \texttt{\_match\_frames}: Feature matching between consecutive frames
    \item \texttt{\_triangulate\_points}: 3D point reconstruction
    \item \texttt{\_estimate\_scale}: Scale recovery for monocular setup
\end{itemize}

\section{Implementation Details}
\subsection{Feature Detection and Matching}
The system uses SIFT features for robust detection and matching:
\begin{lstlisting}[language=python]
self.feature_detector = cv2.SIFT_create(nfeatures=2000)
FLANN_INDEX_KDTREE = 1
index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)
search_params = dict(checks=50)
self.matcher = cv2.FlannBasedMatcher(index_params, search_params)
\end{lstlisting}

\subsection{Pose Estimation}
Camera pose is estimated using the Essential matrix decomposition:
\begin{itemize}
    \item Compute Essential matrix using RANSAC
    \item Decompose into rotation and translation
    \item Verify solution using triangulated points
\end{itemize}

\subsection{Point Cloud Generation}
3D points are reconstructed using triangulation:
\begin{itemize}
    \item Filter points based on triangulation angle
    \item Remove points with large reprojection error
    \item Apply scale estimation for consistent reconstruction
\end{itemize}

\section{Camera Calibration}
\subsection{Calibration Process}
The system includes a camera calibration tool that:
\begin{itemize}
    \item Uses a checkerboard pattern
    \item Captures multiple views
    \item Computes intrinsic parameters
    \item Saves calibration results
\end{itemize}

\subsection{Usage}
To calibrate your camera:
\begin{lstlisting}[language=bash]
python camera_calibration.py
\end{lstlisting}

\section{Usage Guide}
\subsection{Dataset Mode}
To run the system with a dataset:
\begin{lstlisting}[language=python]
# In main.py
use_dataset = True
data_dir = "path/to/dataset"
\end{lstlisting}

\subsection{Live Video Mode}
To run with live video:
\begin{lstlisting}[language=python]
# In main.py
use_dataset = False
\end{lstlisting}

\section{Output and Visualization}
The system generates:
\begin{itemize}
    \item 2D trajectory plot (\texttt{2D\_trajectory.png})
    \item 3D point cloud (\texttt{3D\_map.pcd})
    \item Real-time visualization of tracked features
\end{itemize}

\section{Performance Considerations}
\subsection{Parameters}
Key parameters affecting system performance:
\begin{itemize}
    \item \texttt{min\_matches}: Minimum feature matches (default: 100)
    \item \texttt{min\_triangulation\_angle}: Minimum angle for triangulation (default: 2.0Â°)
    \item \texttt{max\_point\_distance}: Maximum allowed point distance (default: 30.0m)
\end{itemize}

\subsection{Optimization}
Performance optimization strategies:
\begin{itemize}
    \item Feature detection limit
    \item Point cloud filtering
    \item Keyframe selection criteria
\end{itemize}

\section{Troubleshooting}
Common issues and solutions:
\begin{itemize}
    \item Poor tracking: Adjust feature detection parameters
    \item Incorrect scale: Check scale estimation parameters
    \item Missing points: Adjust triangulation filters
\end{itemize}

\section{Future Improvements}
Potential enhancements:
\begin{itemize}
    \item Loop closure detection
    \item Bundle adjustment
    \item Dense reconstruction
    \item Real-time optimization
\end{itemize}

\end{document} 